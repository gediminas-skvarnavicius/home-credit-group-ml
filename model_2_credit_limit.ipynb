{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Credit Card Over Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "import polars as pl\n",
    "from imblearn.pipeline import Pipeline,make_pipeline\n",
    "import auxiliary.transformers as tr\n",
    "import auxiliary.eda_functions as eda\n",
    "from auxiliary.transformers import PolarsColumnTransformer as PCT\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "import auxiliary.tuning as tunes\n",
    "from ray import tune\n",
    "import joblib\n",
    "import numpy as np\n",
    "from BorutaShap import BorutaShap\n",
    "%aimport auxiliary.eda_functions\n",
    "%aimport auxiliary.transformers\n",
    "%aimport auxiliary.tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Imports and Splitting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.read_parquet(\"temp/active_credit_cards.parquet\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(columns=\"IS_OVER_LIMIT\"),\n",
    "    data[\"IS_OVER_LIMIT\"],\n",
    "    test_size=0.3,\n",
    "    stratify=data[\"IS_OVER_LIMIT\"],\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specifying feature types:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_features = []\n",
    "for feature in X_train.select(pl.col(pl.Utf8)).columns:\n",
    "    if X_train[feature].n_unique() == 2:\n",
    "        bool_features.append(feature)\n",
    "\n",
    "cat_features = [\n",
    "    feature\n",
    "    for feature in X_train.select(pl.col(pl.Utf8)).columns\n",
    "    if feature not in bool_features\n",
    "]\n",
    "\n",
    "numeric_features_with_nulls = (\n",
    "    pl.Series(\n",
    "        X_train.select(pl.col(pl.FLOAT_DTYPES), pl.col(pl.INTEGER_DTYPES)).columns\n",
    "    )\n",
    "    .filter(\n",
    "        X_train.select(pl.col(pl.FLOAT_DTYPES), pl.col(pl.INTEGER_DTYPES))\n",
    "        .select(pl.all().is_null().any())\n",
    "        .transpose()\n",
    "        .to_series()\n",
    "    )\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing pipeline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline([])\n",
    "cat_imputers = tr.PolarsColumnTransformer([])\n",
    "for feature in cat_features:\n",
    "    cat_imputers.steps[feature] = PCT.Step(\n",
    "        feature, tr.NotInImputerPolars(min_values=100, fill_value=\"other\"), feature\n",
    "    )\n",
    "preprocessing.steps.append((\"cat_imputers\", cat_imputers))\n",
    "\n",
    "encoders = tr.PolarsColumnTransformer([])\n",
    "for feature in bool_features:\n",
    "    encoders.steps[feature] = PCT.Step(\n",
    "        feature, tr.PolarsOneHotEncoder(drop=True), feature\n",
    "    )\n",
    "for feature in cat_features:\n",
    "    encoders.steps[feature] = PCT.Step(\n",
    "        feature, tr.TargetMeanOrderedLabeler(how=\"label\"), feature\n",
    "    )\n",
    "\n",
    "preprocessing.steps.append((\"encoders\", encoders))\n",
    "\n",
    "scaler=tr.PolarsColumnTransformer([PCT.Step('scaler', MinMaxScaler(), preprocessing.fit_transform(X_train,y_train).columns)])\n",
    "\n",
    "preprocessing.steps.append((\"scaler\", scaler))\n",
    "preprocessing.steps.append((\"null_imputer\",tr.PolarsNullImputer(-1)))\n",
    "\n",
    "feature_remover = tr.FeatureRemover([])\n",
    "preprocessing.steps.append((\"feature_removal\", feature_remover))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model and Oversampling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = LGBMClassifier(n_jobs=1, verbosity=-1)\n",
    "sampler_model = tr.SamplingModelWrapper(model_lgb)\n",
    "full_pipeline = Pipeline([(\"preprocess\", preprocessing), (\"model\", sampler_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Boruta SHAP with different strictness and saving lists of bad features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['temp/model_2_feature_removal.joblib']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_removal_list=[]\n",
    "for i in [20,40,60,80,100]:\n",
    "    model_for_selection_high_reg=LGBMClassifier(verbose=-1,random_state=1,num_leaves=100)\n",
    "    selector=BorutaShap(importance_measure='shap',model=model_for_selection_high_reg,percentile=i)\n",
    "    selector.fit(\n",
    "        full_pipeline[\"preprocess\"]\n",
    "        .fit_transform(X_train, y_train)\n",
    "        .to_pandas(),\n",
    "        y_train.to_pandas(),\n",
    "    )\n",
    "    feature_removal_list.append(copy.deepcopy(selector.features_to_remove.tolist()))\n",
    "joblib.dump(feature_removal_list,'temp/model_2_feature_removal.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the lists of features to remove:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_removal_list=joblib.load(\"temp/model_2_feature_removal.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
